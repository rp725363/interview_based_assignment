Python
Question 1: -
Write a program that takes a string as input, and counts the frequency of each word in the string, there might be repeated characters in the string. Your task is to find the highest frequency and returns the length of the
highest-frequency word.
Note - You have to write at least 2 additional test cases in which your program will run successfully and provide an explanation for the same.
Example input - string = “write write write all the number from from from 1 to 100”
Example output - 5
Explanation - From the given string we can note that the most frequent words are “write” and “from” and
the maximum value of both the values is “write” and its corresponding length is 5

Ans:-

Here we have written function below for finding and count the words in sting to count the length as below :-

def find_length_of_most_frequent_word(input_string):
  words=input_string.split()
  
  freq={}
  for word in words:
    if word in freq:
      freq[word]+=1
    else:
      freq[word]=1
    
  max_freq = max(freq.values())
  max_freq_words=[word for word , freq in freq.items() if freq == max_freq]

  hightest_frequent_word_length=len(max(max_freq_words,key=len))
  return hightest_frequent_word_length

input_string=input("enter a string")
result=find_length_of_most_frequent_word(input_string)
print("this is length of most freqent word :" , result)

First we have defiend a function called "find_length_of_most_frequent_word" and then we initialized that function with taken string as input from user , then we have spilted that given string to words by calling function split into words.
Then we created a blank dictonary to store the words with frequency , then we initialize all words using for loop for words and given if condition to find to find all words.
And then we used "freq" dictonary to count most frequent word in given string by using for loop with if condition and counted the most frequent word using values() function.
Then we counted the word's length using "len()" function and return the same .
After defining function  we have tested out function as giving a string "This is rajendra pandey from ineuron almuny and trying to make people feel in the world".
Testcase1 
After defining function  we have tested out function as giving a string "This is rajendra pandey from ineuron almuny and trying to make people feel in the world".
 we called function "find_length_of_most_frequent_word"  initiliazed with given string and got output "his is length of most freqent word : 8"

Testcase2
And we have given another string "Write a program that takes a string as input, and counts the frequency of each word in the string, there might be repeated characters in the string. Your task is to find the highest frequency and returns the length of the"
And we got output "this is length of most freqent word : 3"

Question 2: -
Consider a string to be valid if all characters of the string appear the same number of times. It is also valid if he can remove just one character at the index in the string, and the remaining characters will occur the same number of times. Given a string, determine if it is valid. If so, return YES , otherwise return NO .
Note - You have to write at least 2 additional test cases in which your program will run successfully and provide
an explanation for the same.
Example input 1 - s = “abc”. This is a valid string because frequencies are { “a”: 1, “b”: 1, “c”: 1 }
Example output 1- YES
Example input 2 - s “abcc”. This string is not valid as we can remove only 1 occurrence of “c”. That leaves
character frequencies of { “a”: 1, “b”: 1 , “c”: 2 }
Example output 2 - NO

Ans:-
Here is how we can write a function with condition to validate the given string by user to check weather all charecto are occuring with same attempt.
from collections import Counter

def string_validation(input_str):
  char_freq=Counter(input_str)
  max_freq=max(char_freq.values())
  freq_count=sum(freq != max_freq for freq in char_freq.values())
  if freq_count <= 1:
    return "YES"
  else:
    return "NO"

input_str=input("enter your string : ")
result=string_validation(input_str)
print(result)

Explanation:-
First we have import counter from collection module of python and then we define the function as below.
In above function "string_validation" we  have counted the frequency of charector and given variable "char-freq" with initializing the counter module .
Then we counted maximum frequency to ensure that if frequencies are same then we will declear the string valid using if condition , else we can decleare invalid as per given guidelines.
Testcase1:-
We have given a string "this is rajendra pandey from ineuron" to test our function and 
and we initiliazed this string with function "string_validation" , and then we printed result after function executed .
And out is "NO"
Testcase2:-
Again we have given string "ganesh" as all charector occured same times in string and our validated and given output as "YES"


Question 3: -
Write a program, which would download the data from the provided link, and then read the data and convert that into properly structured data and return it in Excel format.
Note - Write comments wherever necessary explaining the code written.
Link - https://raw.githubusercontent.com/Biuni/PokemonGO-Pokedex/master/pokedex.json
Data Attributes - id: Identification Number - int num: Number of the
● Pokémon in the official Pokédex - int name: Pokémon name -
● string img: URL to an image of this Pokémon - string type:
● Pokémon type -string height: Pokémon height - float
● weight: Pokémon weight - float candy: type of candy used to evolve Pokémon or
given
● when transferred - string candy_count: the amount of candies required to evolve
- int
● egg: Number of kilometers to travel to hatch the egg - float spawn_chance:
● Percentage of spawn chance (NEW) - float avg_spawns: Number of this
pokemon on 10.000 spawns (NEW) - int
● spawn_time: Spawns most active at the time on this field. Spawn times are the same for all
time zones and are expressed in local time. (NEW) - “minutes: seconds” multipliers:
Multiplier of Combat Power (CP) for calculating the CP after evolution See below - list of int
weakness: Types of
● Pokémon this Pokémon is weak to - list of strings next_evolution: Number and Name of
successive evolutions of Pokémon - list of dict prev_evolution: Number and Name of previous
evolutions of Pokémon - - list of dict





Question 4 -
Write a program to download the data from the link given below and then read the data and convert the into the proper structure and return it as a CSV file.
Link - https://data.nasa.gov/resource/y77d-th95.json
Note - Write code comments wherever needed for code understanding.
Excepted Output Data Attributes
● Name of Earth Meteorite - string id - ID of Earth
● Meteorite - int nametype - string recclass - string
● mass - Mass of Earth Meteorite - float year - Year at which Earth
● Meteorite was hit - datetime format reclat - float recclong - float
● point coordinates - list of int

Ans:- 
Here we have written function to download data from json link as below and we can convert the same in csv file.
import pandas as pd
import requests
def download_data(url):
response=requests.get(url)
json_data=response.json()
data_url="https://data.nasa.gov/resource/y77d-th95.json"
data=download_data(data_url)
 #Then we have created another class for converting processing and converting this data to csv file.and we saved by naming "rajendra,csv"
import pandas as pd
import csv
#first processing the data
df=DataFrame(data)
#For changing "id" to "Id of Earth", we have written below code using pandas dataframe to rename the column.
df.rename(columns={"id":"ID of Earth"},inplace=True)
#then we changes the coulmns name to the standered format in capitalized.
new_heders='Name', 'ID', 'ID_of_Earth', 'Recclass', 'Mass', 'Fall', 'Year',
       'Reclat', 'Reclong', 'Geolocation', 'Computed_region_cbhk_fwbd',
       'Computed_region_nnqa_25f4']
# then we wrote code to read csv file with pandas module as below
df=pd.read_csv('rajendra.csv')
#then we combine 'name' and 'ID' column and created another column named 'ID_of_Earth'
and made datatype string by below code
df['ID']=df['ID'].astype('string')
df['ID_of_Earth']=df['Name']+"_"+df['ID']


Question 5 -
Write a program to download the data from the given API link and then extract the following data with proper formatting
Link - http://api.tvmaze.com/singlesearch/shows?q=westworld&embed=episodes
Note - Write proper code comments wherever needed for the code understanding

import requests

# API endpoint URL
api_url = "http://api.tvmaze.com/singlesearch/shows?q=westworld&embed=episodes"

# Send a GET request to the API
response = requests.get(api_url)

# Check if the request was successful
if response.status_code == 200:
    data = response.json()

    # Extract show information
    show_name = data["name"]
    show_language = data["language"]
    show_genre = data["genres"]
    show_summary = data["summary"]

    print("Show Information:")
    print("Name:", show_name)
    print("Language:", show_language)
    print("Genres:", ", ".join(show_genre))
    print("Summary:", show_summary)

    # Extract episode information
    episodes = data["_embedded"]["episodes"]

    print("\nEpisode Information:")
    for episode in episodes:
        episode_name = episode["name"]
        episode_season = episode["season"]
        episode_number = episode["number"]
        episode_summary = episode["summary"]

        print("Name:", episode_name)
        print("Season:", episode_season)
        print("Number:", episode_number)
        print("Summary:", episode_summary)
        print("----------------------")

else:
    print("Error occurred while accessing the API:", response.status_code)


Excepted Output Data Attributes -
● id - int url - string
df['id']=df['id'].astype('int')
df['url']=df['url'].astype('string')
● name - string season
df['name']=df['name'].astype('string')
● - int number - int
df['season']=df['season'].astype(int)
df['number']=df['number'].astype(int)
● type - string airdate -
df['type']=df['type'].astype('string')
● date format airtime -
df['airdate']=pd.to_datetime(df['airdate'],format='%y-%m-%d'
● 12-hour time format
df['airtime'] = df['airtime'].dt.strftime('%I:%M:%S %p')
● runtime - float
df['runtime']=df['runtime'].astype('float')
● average rating - float
#for changing rating to flot we need to change the column in string first then we import reguler expression for removing charectors from column and then we changed type of data to float with below code .
import re
df['rating']=df['rating'].astype('string')
df['rating']=df['rating'].apply(lambda x:re.sub('\D','',x))
df['rating']=df['rating'].astype('float')
● summary - string
df['summary']=df['summary'].astype('string')
● without html tags
● medium image link - string
df['image']=df['image'].astype('string')
● Original image link - string
df['_links']=df['_links'].astype('string')



Question 6 - Using the data from Question 3, write code to analyze the data and answer the following questions Note 
1.Draw plots to demonstrate the analysis for the following questions for better visualizations.
2. Write code comments wherever required for code understanding
#we wrote below code to extract data from link as guided by question 

import requests
import json

# Make an HTTP GET request to download the JSON file
url = 'https://raw.githubusercontent.com/Biuni/PokemonGO-Pokedex/master/pokedex.json'
response = requests.get(url)

# Load the JSON data
data = json.loads(response.text)

# Extract the desired data from the JSON structure
# Modify this section according to the specific data you want to extract
pokemon_list = data['pokemon']
for pokemon in pokemon_list:
    name = pokemon['name']
    types = pokemon['type']
    print(f"Name: {name}")
    print(f"Types: {types}")
    print('---')

Insights to be drawn -
● Get all Pokemons whose spawn rate is less than 5%
pokemon_less_than_5_percent = [pokemon for pokemon in data['pokemon'] if pokemon['spawn_chance'] < 5]
for pokemon in pokemon_less_than_5_percent:
    name = pokemon['name']
    spawn_rate = pokemon['spawn_chance']
    print(f"Name: {name}")
    print(f"Spawn Rate: {spawn_rate}")
    print('---')

● Get all Pokemons that have less than 4 weaknesses
pokemon_less_than_4_weaknesses = [pokemon for pokemon in data['pokemon'] if len(pokemon['weaknesses']) < 4]
for pokemon in pokemon_less_than_4_weaknesses:
    name = pokemon['name']
    weaknesses = pokemon['weaknesses']
    print(f"Name: {name}")
    print(f"Weaknesses: {weaknesses}")
    print('---')
● Get all Pokemons that have no multipliers at all
pokemon_no_multipliers = [pokemon for pokemon in data['pokemon'] if not pokemon.get('multipliers')]
for pokemon in pokemon_no_multipliers:
    name = pokemon['name']
    print(f"Name: {name}")
    print('---')
● Get all Pokemons that do not have more than 2 evolutions
pokemon_no_mowre_than_2_evolutions = [pokemon for pokemon in data['pokemon'] if len(pokemon.get('next_evolution', [])) <= 2]
for pokemon in pokemon_no_mowre_than_2_evolutions:
    name = pokemon['name']
    print(f"Name: {name}")
    print('---')
● Get all Pokemons whose spawn time is less than 300 seconds.
pokemon_less_than_300_sec = [pokemon for pokemon in data['pokemon'] if pokemon['spawn_time'] < '300']
for pokemon in pokemon_less_than_300_sec:
    name = pokemon['name']
    spawn_time = pokemon['spawn_time']
    print(f"Name: {name}")
    print(f"Spawn Time: {spawn_time}")
    print('---')
Note - spawn time format is "05:32”, so assume “minute: second” format and perform the analysis.
● Get all Pokemon who have more than two types of capabilities
pokemon_more_than_2_capabilities = [pokemon for pokemon in data['pokemon'] if len(pokemon['type']) > 2]
for pokemon in pokemon_more_than_2_capabilities:
    name = pokemon['name']
    capabilities = pokemon['type']
    print(f"Name: {name}")
    print(f"Capabilities: {capabilities}")
    print('---')

Question 7 - Using the data from Question 4, write code to analyze the data and answer the following questions Note -
1. Draw plots to demonstrate the analysis for the following questions for better visualizations
2. Write code comments wherever required for code understanding
#we wrote below code to extract data from link given in question 4 
import requests
import json
import pandas as pd

# Make an HTTP GET request to download the JSON data
url = 'https://data.nasa.gov/resource/y77d-th95.json'
response = requests.get(url)

# Load the JSON data
data = json.loads(response.text)

# Print the number of records in the dataset
num_records = len(data)
print(f"Number of records: {num_records}")

# Perform additional analysis or operations on the data as per your requirements

Insights to be drawn -
● Get all the Earth meteorites that fell before the year 2000
earth_meteorites_before_2000 = [meteorite for meteorite in data if meteorite.get('reclat') and meteorite.get('reclong') and meteorite.get('year') and meteorite['reclat'] != '0' and meteorite['reclong'] != '0' and int(meteorite['year'][:4]) < 2000]
for meteorite in earth_meteorites_before_2000:
    name = meteorite['name']
    year = meteorite['year']
    print(f"Name: {name}")
    print(f"Year: {year}")
    print('---')
● Get all the earth meteorites co-ordinates who fell before the year 1970
coordinates_before_1970 = [(meteorite['reclat'], meteorite['reclong']) for meteorite in data if meteorite.get('reclat') and meteorite.get('reclong') and meteorite.get('year') and meteorite['reclat'] != '0' and meteorite['reclong'] != '0' and int(meteorite['year'][:4]) < 1970]
for lat, lon in coordinates_before_1970:
    print(f"Latitude: {lat}")
    print(f"Longitude: {lon}")
    print('---')

● Assuming that the mass of the earth meteorites was in kg, get all those whose mass was more than 10000kg
meteorites_greater_than_10000kg = [meteorite for meteorite in data if meteorite.get('mass (g)') and float(meteorite['mass (g)']) > 10000]
for meteorite in meteorites_greater_than_10000kg:
    name = meteorite['name']
    mass = float(meteorite['mass (g)']) / 1000  # Convert mass from grams to kilograms
    print(f"Name: {name}")
    print(f"Mass (kg): {mass:.2f}")
    print('---')


Question 8 -Using the data from Question 5, write code the analyze the data and answer the following questions Note -
1. Draw plots to demonstrate the analysis for the following questions and better visualizations
2. Write code comments wherever required for code understanding
#first we written code to extract data from given link in question no.- 5
import requests

# API endpoint URL
api_url = "http://api.tvmaze.com/singlesearch/shows?q=westworld&embed=episodes"

# Send a GET request to the API
response = requests.get(api_url)

# Check if the request was successful
if response.status_code == 200:
    data = response.json()

    # Extract show information
    show_name = data["name"]
    show_language = data["language"]
    show_genre = data["genres"]
    show_summary = data["summary"]

    print("Show Information:")
    print("Name:", show_name)
    print("Language:", show_language)
    print("Genres:", ", ".join(show_genre))
    print("Summary:", show_summary)

    # Extract episode information
    episodes = data["_embedded"]["episodes"]

    print("\nEpisode Information:")
    for episode in episodes:
        episode_name = episode["name"]
        episode_season = episode["season"]
        episode_number = episode["number"]
        episode_summary = episode["summary"]

        print("Name:", episode_name)
        print("Season:", episode_season)
        print("Number:", episode_number)
        print("Summary:", episode_summary)
        print("----------------------")

else:
    print("Error occurred while accessing the API:", response.status_code)

Insights to be drawn -
● Get all the overall ratings for each season and using plots compare the ratings for all the
seasons, like season 1 ratings, season 2, and so on.

import matplotlib.pyplot as plt


season_numbers = []
ratings = []

    # Extract season numbers and ratings from each episode
for episode in episodes:
    season_number = episode['season']
    rating = episode['rating']['average']

    season_numbers.append(season_number)
    ratings.append(float(rating))  # Convert the rating to a float

    # Plot the ratings for each season
    plt.figure(figsize=(8, 6))
    plt.plot(season_numbers, ratings, marker='o', linestyle='-', color='blue')
    plt.xlabel('Season')
    plt.ylabel('Rating')
    plt.title(f'{show_name} - Ratings by Season')
    plt.xticks(season_numbers)
    plt.grid(True)
    plt.show()
else:
    print("Failed to retrieve data from the API.")
● Get all the episode names, whose average rating is more than 8 for every season
 # Extract information from the data
show_name = data['name']
episodes = data['_embedded']['episodes']

    # Iterate through each season
for season in range(1, max([episode['season'] for episode in episodes]) + 1):
    print(f"Season {season}:")

        # Filter episodes with average rating > 8 for the current season
    season_episodes = [
        episode['name']
            for episode in episodes
            if episode['season'] == season and float(episode['rating']['average']) > 8
        ]

        # Print episode names for the current season
if season_episodes:
    for episode_name in season_episodes:
        print(episode_name)
else:
    print("No episodes with average rating > 8")

    print()  # Print an empty line between seasons


● Get all the episode names that aired before May 2019
from datetime import datetime

show_name = data['name']
episodes = data['_embedded']['episodes']

    # Convert the date string to a datetime object
target_date = datetime.strptime("May 2019", "%B %Y")

    # Iterate through each episode
for episode in episodes:
    air_date = datetime.strptime(episode['airdate'], "%Y-%m-%d")

        # Check if the episode aired before May 2019
if air_date < target_date:
    episode_name = episode['name']
    print(episode_name)

episode['name']



● Get the episode name from each season with the highest and lowest rating
    show_name = data['name']
    episodes = data['_embedded']['episodes']

    # Initialize dictionaries to store highest and lowest ratings for each season
    highest_ratings = {}
    lowest_ratings = {}

    # Iterate through each episode
    for episode in episodes:
        season_number = episode['season']
        episode_name = episode['name']
        episode_rating = float(episode['rating']['average'])

        # Update highest and lowest ratings for each season
        if season_number not in highest_ratings or episode_rating > highest_ratings[season_number]['rating']:
            highest_ratings[season_number] = {'episode': episode_name, 'rating': episode_rating}
        if season_number not in lowest_ratings or episode_rating < lowest_ratings[season_number]['rating']:
            lowest_ratings[season_number] = {'episode': episode_name, 'rating': episode_rating}

    # Print the episode name with the highest and lowest rating for each season
    for season_number in highest_ratings:
        print(f"Season {season_number}:")
        print("Highest Rated Episode:", highest_ratings[season_number]['episode'], "| Rating:",
              highest_ratings[season_number]['rating'])
        print("Lowest Rated Episode:", lowest_ratings[season_number]['episode'], "| Rating:",
              lowest_ratings[season_number]['rating'])
        print()  # Print an empty line between seasons
● Get the summary for the most popular ( ratings ) episode in every season
    show_name = data['name']
    episodes = data['_embedded']['episodes']

    # Initialize dictionaries to store highest ratings and episode summaries for each season
    highest_ratings = {}
    episode_summaries = {}

    # Iterate through each episode
    for episode in episodes:
        season_number = episode['season']
        episode_name = episode['name']
        episode_rating = float(episode['rating']['average'])
        episode_summary = episode['summary']

        # Update highest rating and episode summary for each season
        if season_number not in highest_ratings or episode_rating > highest_ratings[season_number]['rating']:
            highest_ratings[season_number] = {'episode': episode_name, 'rating': episode_rating}
            episode_summaries[season_number] = episode_summary

    # Print the summary for the most popular episode in each season
    for season_number in episode_summaries:
        print(f"Season {season_number}:")
        print("Episode Name:", highest_ratings[season_number]['episode'])
        print("Summary:", episode_summaries[season_number])
        print()  # Print an empty line between seasons

Question 9 -
Write a program to read the data from the following link, perform data analysis and answer the following questions
Note -
1. Write code comments wherever required for code understanding
Link - https://data.wa.gov/api/views/f6w7-q2d2/rows.csv?accessType=DOWNLOAD
# We hve written first code for retriving data from given link and then given to pandas for set and allign the data as below:
import pandas as pd

# Read data from the link
data_url = "https://data.wa.gov/api/views/f6w7-q2d2/rows.csv?accessType=DOWNLOAD"
df = pd.read_csv(data_url)

Insights to be drawn -
● Get all the cars and their types that do not qualify for clean alternative fuel vehicle
non_qualifying_cars = df[df['Clean Alternative Fuel Vehicle (CAFV) Eligibility'].str.lower() != 'Clean Alternative Fuel Vehicle Eligible']
non_qualifying_cars = non_qualifying_cars[['Electric Vehicle Type']]
print(non_qualifying_cars)
#this way we will get the output of whole type of vehcle that are not clean alternative.

● Get all TESLA cars with the model year, and model type made in Bothell City.
tesla_bothell_cars = df[(df['Make'] == 'TESLA') & (df['City'] == 'BOTHELL')]
tesla_bothell_cars = tesla_bothell_cars[['Model Year', 'Model', 'City']]
print(tesla_bothell_cars)
● Get all the cars that have an electric range of more than 100, and were made after
2015
df['Electric Range'] = pd.to_numeric(df['Electric Range'], errors='coerce')
filtered_cars = df[(df['Electric Range'] > 100) & (df['Model Year'] > 2015)]
lal=pd.DataFrame(filtered_cars)

● Draw plots to show the distribution between city and electric vehicle type

electric_vehicles = df[df['Electric Vehicle Type'].str.lower().str.contains('electric')]

# Group by city and electric vehicle type
grouped_data = electric_vehicles.groupby(['City', 'Electric Vehicle Type']).size().unstack()

# Plot the distribution
grouped_data.plot(kind='bar', stacked=True, figsize=(16, 8))
plt.xlabel('City')
plt.ylabel('Count')
plt.title('Distribution of Electric Vehicle Types by City')
plt.legend(title='Electric Vehicle Type')
plt.xticks(rotation=45)
plt.show()


Question 10 -
Write a program to count the number of verbs, nouns, pronouns, and adjectives in a given particular phrase or paragraph, and return their respective count as a dictionary.
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag

def count_pos_tags(text):
    # Download necessary resources
    nltk.download('punkt')
    nltk.download('averaged_perceptron_tagger')

    # Tokenize the text into sentences and words
    sentences = sent_tokenize(text)
    words = [word_tokenize(sentence) for sentence in sentences]

    # Perform POS tagging on the words
    tagged_words = [pos_tag(word) for word in words]

    # Initialize counts
    verb_count = 0
    noun_count = 0
    pronoun_count = 0
    adjective_count = 0

    # Count the POS tags
    for tagged_sentence in tagged_words:
        for word, tag in tagged_sentence:
            if tag.startswith('VB'):
                verb_count += 1
            elif tag.startswith('NN'):
                noun_count += 1
            elif tag.startswith('PRP'):
                pronoun_count += 1
            elif tag.startswith('JJ'):
                adjective_count += 1

    # Create a dictionary with the counts
    pos_counts = {
        'Verbs': verb_count,
        'Nouns': noun_count,
        'Pronouns': pronoun_count,
        'Adjectives': adjective_count
    }

    return pos_counts

# Example usage
text = "The quick brown fox jumps over the lazy dog."
counts = count_pos_tags(text)
print(counts)

Note -
1. Write code comments wherever required for code
2. You have to write at least 2 additional test cases in which your program will run successfully and provide an explanation for the same.

Testcase -1:-
As per above code ,  first we have given sentenace as string "The quick brown fox jumps over the lazy dog." and output came was- {'Verbs': 1, 'Nouns': 3, 'Pronouns': 0, 'Adjectives': 2}.

Testcase- 2:-
In above code we have given a paaragraph "Once upon a time there was a lion called jagira. Jagira was very dangerous lion for the forest and for all living animals. All animals were afraid of him" and output was - {'Verbs': 4, 'Nouns': 9, 'Pronouns': 1, 'Adjectives': 2}








Statistics
Q-1. A university wants to understand the relationship between the SAT scores of its applicants and their college GPA. They collect data on 500 students, including their SAT scores (out of 1600) and their college GPA (on a 4.0 scale). They find that the correlation coefficient between SAT scores and college GPA is 0.7. What does this correlation coefficient indicate about the relationship between SAT scores and college GPA?

Answer:-
The correlation coefficient ranges from -1 to 1, where values closer to 1 represent a strong positive correlation. In this case, the correlation coefficient of 0.7 suggests that as SAT scores increase, there is a tendency for college GPAs to also increase.
It's important to note that correlation does not imply causation. While there is a strong correlation between SAT scores and college GPA, it does not necessarily mean that higher SAT scores directly cause higher GPAs. Other factors, such as study habits, motivation, and personal circumstances, can also influence college performance. The correlation coefficient simply indicates the strength and direction of the linear relationship between the two variables (SAT scores and college GPA) in the given dataset.

Q-2. 
Consider a dataset containing the heights (in centimeters) of 1000 individuals. The mean height is 170 cm with a standard deviation of 10 cm. The dataset is approximately normally distributed, and its skewness is approximately zero. Based on this information, answer the following questions:
a. What percentage of individuals in the dataset have heights between 160 cm
and 180 cm?
Ans- The mean height is 170 cm with a standard deviation of 10 cm. To find the percentage of individuals with heights between 160 cm and 180 cm, we need to calculate the area under the normal curve between these two values.
We can convert the given values into z-scores, which represent the number of standard deviations an individual's height is from the mean. The z-score formula is:
z = (x - μ) / σ
where x is the height, μ is the mean height, and σ is the standard deviation.
For the lower bound of 160 cm:
z1 = (160 - 170) / 10 = -1

For the upper bound of 180 cm:
z2 = (180 - 170) / 10 = 1
We can then use a standard normal distribution table or a calculator to find the area under the curve between -1 and 1.
Using the standard normal distribution table or calculator, the area between -1 and 1 is approximately 0.6827 or 68.27%.
so as per given insights approximetly 68% peoples heights are between 160cm to 180 cm

b. If we randomly select 100 individuals from the dataset, what is the probability
that their average height is greater than 175 cm?
Ans:-
Here we can make use of the Central Limit Theorem. The Central Limit Theorem states that when a large sample is drawn from any population with a finite mean and standard deviation, the distribution of the sample means approaches a normal distribution.
Given that the dataset is approximately normally distributed with a mean height of 170 cm and a standard deviation of 10 cm, we can assume that the sample means of randomly selected groups of 100 individuals will also be normally distributed. The mean of the sample means will be the same as the population mean, which is 170 cm. However, the standard deviation of the sample means, also known as the standard error, can be calculated as:
Standard Error = Population Standard Deviation / √Sample Size
Standard Error = 10 / √100 = 1 cm
and while calculating probaility using z-score formula as below :-
z = (x - μ) / σ
z = (175 - 170) / 1 = 5
Using a standard normal distribution table or calculator, we can find the area under the curve to the right of z = 5. This area represents the probability that the average height is greater than 175 cm.
So practically probability of 170cm heights is zero in random taken 100 sample data.

c. Assuming the dataset follows a normal distribution, what is the z-score corresponding to a height of 185 cm?
Ans:-
Here how we can calculate the z-score of a normal distrubution for to considering 180cm heights.
z = (x - μ) / σ
where x is heights, μ is mean of the height and σ is standered daviation.
Substituting the values into the formula:
z = (185 - 170) / 10 = 15 / 10 = 1.5
therefore the z-score cosidering 185cm height is 1.5.

d. We know that 5% of the dataset has heights below a certain value. What is
the approximate height corresponding to this threshold?
Ans:-
To find the approximate height corresponding to the threshold below which 5% of the dataset falls, we can use the z-score associated with a cumulative probability of 5% in the standard normal distribution.
Using a standard normal distribution table or a calculator, the z-score corresponding to a cumulative probability of 5% (0.05) is approximately -1.645.
To find the approximate height, we can rearrange the z-score formula:
z = (x - μ) / σ
-1.645 = (x - 170) / 10
x - 170 = -1.645 * 10
x - 170 = -16.45
x ≈ 170 - 16.45
x ≈ 153.55
Therefore, the approximate height corresponding to the threshold below which 5% of the dataset falls is approximately 153.55 cm.

e. Calculate the coefficient of variation (CV) for the dataset.
Ans:-
To calculate the coefficient of the vaiation for the dataset we can us use below foormula of coefficient of variation as below:-
CV = (Standard Deviation / Mean) * 100
CV = (10 / 170) * 100
CV ≈ 5.88%
So coefficient of variation of the dataset is approximatly 5.88%.

f. Calculate the skewness of the dataset and interpret the result
Ans:-
As per given term we have generated data and found median which is 170.225 and we can calculate the skewness of the dataset using the formula:
Skewness = (3 * (Mean - Median)) / Standard Deviation
Using the given values:
Mean = 170 cm
Median = 170.225 cm
Standard Deviation = 10 cm
Skewness = (3 * (Mean - Median)) / Standard Deviation
= (3 * (170 - 170.225)) / 10
= (3 * (-0.225)) / 10
= -0.675 / 10
= -0.0675
with the given median of 170.225 cm and assuming a standard deviation of 10 cm, the skewness of the dataset is approximately -0.0675.

Q-3. 
Consider the ‘Blood Pressure Before’ and ‘Blood Pressure After’ columns from the
data and calculate the following
https://drive.google.com/file/d/1mCjtYHiX--mMUjicuaP2gH3k-SnFxt8Y/view?usp=share_
a. Measure the dispersion in both and interpret the results.
Variance of Blood Pressure Before:
Calculate the mean: (120 + 130 + 125 + 140 + 115) / 5 = 126
Calculate the squared differences from the mean:
(120 - 126)^2, (130 - 126)^2, (125 - 126)^2, (140 - 126)^2, (115 - 126)^2
Calculate the average of the squared differences: (89 + 16 + 1 + 196 + 121) / 5 = 84.6
Variance of Blood Pressure After:
Calculate the mean: (115 + 125 + 130 + 120 + 135) / 5 = 125
Calculate the squared differences from the mean:
(115 - 125)^2, (125 - 125)^2, (130 - 125)^2, (120 - 125)^2, (135 - 125)^2
Calculate the average of the squared differences: (100 + 0 + 25 + 25 + 100) / 5 = 50

b. Calculate mean and 5% confidence interval and plot it in a graph
# this how we calculated mean values
np.mean(df)
# this is hoe we calculated stendered error.
std_error = np.std(df) / np.sqrt(len(df))
std_error
c. Calculate the Mean absolute deviation and Standard deviation and interpret
the results.
mad = np.mean(np.abs(df - mean))
sd = np.std(df)
print(mad)
print(sd)
# and output was
#mad 
Patient ID                       25.0000
 Blood Pressure Before (mmHg)     5.7118
 Blood Pressure After (mmHg)      5.9000
dtype: float64
sd
Patient ID                       28.866070
 Blood Pressure Before (mmHg)     6.565204
 Blood Pressure After (mmHg)      6.853495
dtype: float64
d. Calculate the correlation coefficient and check the significance of it at 1% level
of significance.


Q-4.
 A group of 20 friends decide to play a game in which they each write a number
between 1 and 20 on a slip of paper and put it into a hat. They then draw one slip of paper
at random. What is the probability that the number on the slip of paper is a perfect square
(i.e., 1, 4, 9, or 16)?
Ans:-
we need to determine the number of favorable outcomes (the slips of paper with perfect square numbers) and the total number of possible outcomes (all slips of paper with numbers between 1 and 20).
The total number of possible outcomes is 20 since there are 20 slips of paper in the hat.The number of favorable outcomes is the number of perfect squares between 1 and 20, which are 1, 4, 9, and 16. So there are 4 favorable outcomes.
The probability of an event is given by the ratio of favorable outcomes to total outcomes.  Probability = Number of Favorable Outcomes / Number of Total Outcomes
Applying this to the given scenario:
Probability of drawing a perfect square = 4 / 20 = 0.2
Therefore, the probability of drawing a perfect square number from the hat is 0.2 or 20%.

Q-5. A certain city has two taxi companies: Company A has 80% of the taxis and
Company B has 20% of the taxis. Company A's taxis have a 95% success rate for picking
up passengers on time, while Company B's taxis have a 90% success rate. If a randomly
selected taxi is late, what is the probability that it belongs to Company A?

Ans:-
Bayes' theorem allows us to calculate the probability of an event given information about related events.
Let's define the following events:
A: Taxi belongs to Company A
B: Taxi is late
We are given the following probabilities:
P(A) = 0.8 (Company A has 80% of the taxis)
P(B|A) = 0.05 (Company A's taxis have a 95% success rate, so the probability of being late is 1 - 0.95 = 0.05)
P(B|not A) = 0.1 (Company B's taxis have a 90% success rate, so the probability of being late is 1 - 0.9 = 0.1)
We want to calculate P(A|B), the probability that a randomly selected late taxi belongs to Company A.
According to Bayes' theorem:
P(A|B) = (P(B|A) * P(A)) / P(B)
To calculate P(B), we need to consider all possible scenarios where a taxi can be late:
P(B) = P(B|A) * P(A) + P(B|not A) * P(not A)
Since there are only two companies (A and not A), P(not A) = 1 - P(A).
Let's calculate the probabilities:
P(not A) = 1 - P(A) = 1 - 0.8 = 0.2
P(B) = P(B|A) * P(A) + P(B|not A) * P(not A) = 0.05 * 0.8 + 0.1 * 0.2 = 0.04 + 0.02 = 0.06
Now we can calculate P(A|B):
P(A|B) = (P(B|A) * P(A)) / P(B) = (0.05 * 0.8) / 0.06 = 0.04 / 0.06 = 0.6667
Therefore, the probability that a randomly selected late taxi belongs to Company A is approximately 0.6667 or 66.67%.

Q-6. A pharmaceutical company is developing a drug that is supposed to reduce blood
pressure. They conduct a clinical trial with 100 patients and record their blood
pressure before and after taking the drug. The company wants to know if the change
in blood pressure follows a normal distribution.
